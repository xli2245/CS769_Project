dataset:
  root: 'data'  # not needed if using pre-extracted bottom-up features
  task: 3

text-model:
  name: 'bert' # 'bert', 'albert', 'roberta', 'deberta'
  pretrain: './model/bert-base-uncased' # './model/bert-base-uncased', './model/albert-base-v2', './model/roberta-base', './model/deberta-base'
  word-dim: 768
  extraction-hidden-layer: 10
  fine-tune: False

image-model:
  name: 'resnet50'
  enabled: True
  fine-tune: False
  feat-dim: 2048
  grid: [7, 7]

model:
  name: 'dual-transformer'
  embed-dim: 1024
  feedforward-dim: 1024
  num-encoder-layers: 4
  num-decoder-layers: 4
  model-fusion: 'concat' # average, weighted, concat, MLP

training:
  balanced-sampling: False
  lr: 0.00005 
  pretrained-modules-lr: 0.000001  # learning rate for early pretrained layers
  grad-clip: 2.0
  bs: 8
  scheduler: 'steplr'
  milestones: [40]
  gamma: 0.1